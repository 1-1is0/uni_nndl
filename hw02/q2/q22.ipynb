{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                0: \"T-shirt/Top\",\n",
    "                1: \"Trouser\",\n",
    "                2: \"Pullover\",\n",
    "                3: \"Dress\",\n",
    "                4: \"Coat\", \n",
    "                5: \"Sandal\", \n",
    "                6: \"Shirt\",\n",
    "                7: \"Sneaker\",\n",
    "                8: \"Bag\",\n",
    "                9: \"Ankle Boot\"\n",
    "                }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "          \n",
    "class Fashion_mnist_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Fashion_mnist_CNN, self).__init__()\n",
    "    \n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        # nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    )\n",
    "        # self.drop1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2)\n",
    "        )\n",
    "        # self.drop3 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.drop4 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=1600, out_features=10)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.drop5 = nn.Dropout(0.5)\n",
    "        #self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        # self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        #out = self.drop1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.layer3(out)\n",
    "        #out = self.drop1(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.drop4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.soft(out)\n",
    "        out = self.drop5(out)\n",
    "        #out = self.fc2(out)\n",
    "        # out = self.fc3(out)\n",
    "        \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion_mnist_CNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop2): Dropout(p=0.25, inplace=False)\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop4): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=1600, out_features=10, bias=True)\n",
      "  (soft): Softmax(dim=1)\n",
      "  (drop5): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# plt.imshow(trainset[0].numpy().squeeze(), cmap='gray_r');  \n",
    "model1 = Fashion_mnist_CNN()\n",
    "model1.to(device)\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "# Lists for visualization of loss and accuracy \n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "# Lists for knowing classwise accuracy\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "for images, labels in trainloader:\n",
    "        # Transfering images and labels to GPU if available\n",
    "        images, labels = images.to(device), labels.to(device)        \n",
    "        # Forward pass \n",
    "        outputs = model1(images)\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Initializing a gradient as 0 so there is no mixing of gradient among the batches\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Propagating the error backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizing the parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "        count += 1\n",
    "        # Testing the model\n",
    "    \n",
    "if not (count % 50):    # It's same as \"if count % 50 == 0\"\n",
    "            total = 0\n",
    "            correct = 0\n",
    "        \n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                labels_list.append(labels)\n",
    "            \n",
    "                outputs = model1(images)\n",
    "            \n",
    "                predictions = torch.max(outputs, 1)[1].to(device)\n",
    "                predictions_list.append(predictions)\n",
    "                correct += (predictions == labels).sum()\n",
    "            \n",
    "                total += len(labels)\n",
    "            \n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model1\n",
    "criterion = error\n",
    "data_loader = {\n",
    "    \"train\": trainloader,\n",
    "    \"val\": testloader,\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"train\": len(trainset),\n",
    "    \"val\": len(testset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_curve(current_epoch, optimizer_name, loss_name, res):\n",
    "    x_epoch = list(range(current_epoch))\n",
    "    loss_train = res[\"loss_train\"]\n",
    "    loss_val = res[\"loss_val\"]\n",
    "    plt.plot(x_epoch, loss_train, 'bo-', label='train')\n",
    "    plt.plot(x_epoch, loss_val, 'ro-', label='val')\n",
    "\n",
    "    if current_epoch == 0:\n",
    "        plt.legend()\n",
    "    os.makedirs(\"loss_graphs\", exist_ok=True)\n",
    "    plt.savefig(os.path.join('./loss_graphs',\n",
    "                f'train_{optimizer_name}_{loss_name}.jpg'))\n",
    "\n",
    "def train(epochs=20):\n",
    "    path = \"model\"\n",
    "    state_file_name = f\"{path}/state-{net._get_name()}-optimizer-{optimizer.__class__.__name__}-loss-{criterion.__class__.__name__}.pth\"\n",
    "    state_res = {}\n",
    "\n",
    "    print(state_file_name, end=\" \")\n",
    "    if os.path.exists(state_file_name):\n",
    "        print(\"exist\")\n",
    "        state = torch.load(state_file_name)\n",
    "        net.load_state_dict(state[\"state_dict\"])\n",
    "        optimizer.load_state_dict(state[\"optimizer\"])\n",
    "        state_res = state[\"res\"]\n",
    "\n",
    "    else:\n",
    "        print(\"Not exist\")\n",
    "    res = {\n",
    "        \"loss_train\": state_res.get(\"loss_train\", []),\n",
    "        \"loss_val\": state_res.get(\"loss_val\", []),\n",
    "        \"epoch\": state_res.get(\"epoch\", 0),\n",
    "    }\n",
    "    res[\"epoch\"]\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(res[\"epoch\"]+1, epochs):\n",
    "        running_loss = 0.0\n",
    "        phase_loss = 0\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                net.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                net.train(False)  # Set model to evaluate mode\n",
    "            for i, data in enumerate(data_loader[phase], 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                now_batch_size = labels.size()[0]\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                # print statistics\n",
    "                phase_loss += loss.item() * now_batch_size\n",
    "                running_loss += loss.item()\n",
    "                if i % 200 == 199 and phase == \"train\":\n",
    "                    print(\n",
    "                        f'[{epoch}, {i + 1:5d}] loss: {running_loss / 2000:.4f}')\n",
    "                    running_loss = 0.0\n",
    "            phase_loss = phase_loss / dataset_sizes[phase]\n",
    "            # y_loss[phase].append(phase_loss)\n",
    "            res[f\"loss_{phase}\"].append(phase_loss)\n",
    "            res[\"epoch\"] = epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch} loss: {res['loss_train'][-1]:.8f} val: {res['loss_val'][-1]:.8f}\")\n",
    "        draw_curve(epoch, optimizer_name=optimizer.__class__.__name__,\n",
    "                   loss_name=criterion.__class__.__name__, res=res)\n",
    "\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": net.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"res\": res\n",
    "        }\n",
    "        torch.save(state, state_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/state-Fashion_mnist_CNN-optimizer-Adam-loss-CrossEntropyLoss.pth Not exist\n",
      "[1,   200] loss: 0.1739\n",
      "[1,   400] loss: 0.1739\n",
      "Epoch 1 loss: 1.74068548 val: 1.65403546\n",
      "[2,   200] loss: 0.1733\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\u001b[39m55\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [12], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     58\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.4217670029681838\n",
      "pre 0.4454874066225275\n",
      "acc 0.4265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for a, b in zip(labels, predicted):\n",
    "            # print(a.item(), b.item())\n",
    "            y_true.append(a.item())\n",
    "            y_pred.append(b.item())\n",
    "        # break\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        # _, predicted = torch.max(outputs.data, 1)\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"f1\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "# %%\n",
    "print(\"pre\", precision_score(y_true, y_pred, average=\"macro\"))\n",
    "# %%\n",
    "print(\"acc\", accuracy_score(y_true, y_pred))\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "375aa2ef0b269be7ab695ea0a4cd456f8c66fb0a772baa2b80308fc61f48a1f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
