{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "                0: \"T-shirt/Top\",\n",
    "                1: \"Trouser\",\n",
    "                2: \"Pullover\",\n",
    "                3: \"Dress\",\n",
    "                4: \"Coat\", \n",
    "                5: \"Sandal\", \n",
    "                6: \"Shirt\",\n",
    "                7: \"Sneaker\",\n",
    "                8: \"Bag\",\n",
    "                9: \"Ankle Boot\"\n",
    "                }\n",
    "    input = (label.item() if type(label) == torch.Tensor else label)\n",
    "    return output_mapping[input]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "          \n",
    "class FashionCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "    \n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    )\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.drop2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.drop3 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2),\n",
    "        nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.drop4 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64, out_features=10)\n",
    "        self.soft=nn.Softmax(dim=1)\n",
    "        self.drop5 = nn.Dropout(0.25)\n",
    "        #self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        # self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.drop1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.drop2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.drop3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.drop4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.soft(out)\n",
    "        out = self.drop5(out)\n",
    "        #out = self.fc2(out)\n",
    "        # out = self.fc3(out)\n",
    "        \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop1): Dropout(p=0.25, inplace=False)\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop2): Dropout(p=0.25, inplace=False)\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop3): Dropout(p=0.25, inplace=False)\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (drop4): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (soft): Softmax(dim=1)\n",
      "  (drop5): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# plt.imshow(trainset[0].numpy().squeeze(), cmap='gray_r');  \n",
    "model = FashionCNN()\n",
    "model.to(device)\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.002\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model\n",
    "criterion = error\n",
    "data_loader = {\n",
    "    \"train\": trainloader,\n",
    "    \"val\": testloader,\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"train\": len(trainset),\n",
    "    \"val\": len(testset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_curve(current_epoch, optimizer_name, loss_name, res):\n",
    "    x_epoch = list(range(current_epoch))\n",
    "    loss_train = res[\"loss_train\"]\n",
    "    loss_val = res[\"loss_val\"]\n",
    "    plt.plot(x_epoch, loss_train, 'bo-', label='train')\n",
    "    plt.plot(x_epoch, loss_val, 'ro-', label='val')\n",
    "\n",
    "    if current_epoch == 0:\n",
    "        plt.legend()\n",
    "    os.makedirs(\"loss_graphs\", exist_ok=True)\n",
    "    plt.savefig(os.path.join('./loss_graphs',\n",
    "                f'train_{optimizer_name}_{loss_name}.jpg'))\n",
    "\n",
    "\n",
    "def train(epochs=20):\n",
    "    path = \"model\"\n",
    "    state_file_name = f\"{path}/state-{net._get_name()}-optimizer-{optimizer.__class__.__name__}-loss-{criterion.__class__.__name__}.pth\"\n",
    "    state_res = {}\n",
    "\n",
    "    print(state_file_name, end=\" \")\n",
    "    if os.path.exists(state_file_name):\n",
    "        print(\"exist\")\n",
    "        state = torch.load(state_file_name)\n",
    "        net.load_state_dict(state[\"state_dict\"])\n",
    "        optimizer.load_state_dict(state[\"optimizer\"])\n",
    "        state_res = state[\"res\"]\n",
    "\n",
    "    else:\n",
    "        print(\"Not exist\")\n",
    "    res = {\n",
    "        \"loss_train\": state_res.get(\"loss_train\", []),\n",
    "        \"loss_val\": state_res.get(\"loss_val\", []),\n",
    "        \"epoch\": state_res.get(\"epoch\", 0),\n",
    "    }\n",
    "    res[\"epoch\"]\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(res[\"epoch\"]+1, epochs):\n",
    "        running_loss = 0.0\n",
    "        phase_loss = 0\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                net.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                net.train(False)  # Set model to evaluate mode\n",
    "            for i, data in enumerate(data_loader[phase], 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                now_batch_size = labels.size()[0]\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                # print statistics\n",
    "                phase_loss += loss.item() * now_batch_size\n",
    "                running_loss += loss.item()\n",
    "                if i % 200 == 199 and phase == \"train\":\n",
    "                    print(\n",
    "                        f'[{epoch}, {i + 1:5d}] loss: {running_loss / 2000:.4f}')\n",
    "                    running_loss = 0.0\n",
    "            phase_loss = phase_loss / dataset_sizes[phase]\n",
    "            # y_loss[phase].append(phase_loss)\n",
    "            res[f\"loss_{phase}\"].append(phase_loss)\n",
    "            res[\"epoch\"] = epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch} loss: {res['loss_train'][-1]:.8f} val: {res['loss_val'][-1]:.8f}\")\n",
    "        draw_curve(epoch, optimizer_name=optimizer.__class__.__name__,\n",
    "                   loss_name=criterion.__class__.__name__, res=res)\n",
    "\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": net.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"res\": res\n",
    "        }\n",
    "        torch.save(state, state_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/state-FashionCNN-optimizer-Adam-loss-CrossEntropyLoss.pth Not exist\n",
      "[1,   200] loss: 0.1843\n",
      "[1,   400] loss: 0.1724\n",
      "Epoch 1 loss: 1.77296622 val: 1.66359962\n",
      "[2,   200] loss: 0.1699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\u001b[39m7\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [8], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     54\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     56\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [5], line 52\u001b[0m, in \u001b[0;36mFashionCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(out)\n\u001b[1;32m     51\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop3(out)\n\u001b[0;32m---> 52\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(out)\n\u001b[1;32m     53\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop4(out)\n\u001b[1;32m     54\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/uni/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1253\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m-> 1253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m:\n\u001b[1;32m   1254\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m   1255\u001b[0m         \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _parameters:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.6251775804889689\n",
      "pre 0.6293084691375496\n",
      "acc 0.6239\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for a, b in zip(labels, predicted):\n",
    "            # print(a.item(), b.item())\n",
    "            y_true.append(a.item())\n",
    "            y_pred.append(b.item())\n",
    "        # break\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        # _, predicted = torch.max(outputs.data, 1)\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"f1\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "# %%\n",
    "print(\"pre\", precision_score(y_true, y_pred, average=\"macro\"))\n",
    "# %%\n",
    "print(\"acc\", accuracy_score(y_true, y_pred))\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "375aa2ef0b269be7ab695ea0a4cd456f8c66fb0a772baa2b80308fc61f48a1f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
